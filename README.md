
# DevChat CLI â€“ SpÃ©cialiser Ollama pour le DÃ©veloppement

## ğŸ¯ Objectif

CrÃ©er un petit **chat CLI local** spÃ©cialisÃ© pour le dÃ©veloppement avec un modÃ¨le Ollama local (`deepseek-r1:8b`), combinÃ© Ã  un RAG sur des documents de code.

---

## ğŸ› ï¸ DÃ©pendances Ã  installer

```bash
curl -fsSL https://ollama.com/install.sh | sh
python3 -m venv dev-chat
source devchat/bin/activate
pip install --upgrade pip
pip install llama-index llama-index-llms-ollama rich
```

---

## ğŸ“ Structure du projet recommandÃ©e

```
ton-projet/
â”œâ”€â”€ devchat.py
â””â”€â”€ code_docs/
    â”œâ”€â”€ DOCKER.md
    â”œâ”€â”€ react_patterns.md
    â”œâ”€â”€ backend_tips.md
    â”œâ”€â”€ cheatsheet.txt
    â””â”€â”€ ...
```

---

## ğŸ§  Description du script

- Charge tous les fichiers dans `code_docs/`
- CrÃ©e un index vectoriel (via `llama-index`)
- Connecte ton modÃ¨le local via `ollama`
- Utilise un prompt systÃ¨me spÃ©cialisÃ© dÃ©veloppeur
- Permet de discuter directement via terminal


---

## âœ… Lancement du chat

```bash
python devchat.py
```

---

## ğŸ’¬ Exemples de questions

- "Comment configurer Docker pour un projet Node.js ?"
- "Quel est un bon pattern pour une API REST Express ?"
- "Comment organiser un projet React avec Redux Toolkit ?"
- "Donne un exemple de middleware en Express"

---

## ğŸš€ Options possibles Ã  ajouter

- Interface graphique (`textual`, `gradio`, `streamlit`)
- Historique de conversation sauvegardÃ©
- Analyse directe de code avec coloration syntaxique
