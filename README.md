
# DevChat CLI â€“ SpÃ©cialiser Ollama pour le DÃ©veloppement

## ðŸŽ¯ Objectif

CrÃ©er un petit **chat CLI local** spÃ©cialisÃ© pour le dÃ©veloppement avec un modÃ¨le Ollama local (`deepseek-coder:6.7b`).

---

## ðŸ› ï¸ DÃ©pendances Ã  installer

```bash
curl -fsSL https://ollama.com/install.sh | sh
python3 -m venv dev-chat
source dev-chat/bin/activate
pip install --upgrade pip
pip install llama-index llama-index-llms-ollama rich
```

---

## ðŸ“ Structure du projet recommandÃ©e

```
ton-projet/
â”œâ”€â”€ devchat.py
â”œâ”€â”€ lancement.sh
â”œâ”€â”€ README.md
â””â”€â”€ code_docs/
    â”œâ”€â”€ DOCKER.md
    â”œâ”€â”€ react_patterns.md
    â”œâ”€â”€ backend_tips.md
    â”œâ”€â”€ cheatsheet.txt
    â””â”€â”€ ...
```

---

## ðŸ§  Description du script

- Charge tous les fichiers dans `code_docs/`
- CrÃ©e un index vectoriel (via `llama-index`)
- Connecte ton modÃ¨le local via `ollama`
- Utilise un prompt systÃ¨me spÃ©cialisÃ© dÃ©veloppeur
- Permet de discuter directement via terminal


---

## âœ… Lancement du chat

```bash
python devchat.py
# OU
./lancement.sh
# OU clique droit sur lancement.sh ---> Ã©xÃ©cuter comme un programme ENJOY :)
```

---

## ðŸ’¬ Exemples de questions

- "Comment configurer Docker pour un projet Node.js ?"
- "Quel est un bon pattern pour une API REST Express ?"
- "Comment organiser un projet React avec Redux Toolkit ?"
- "Donne un exemple de middleware en Express"

---

## ðŸš€ Options possibles Ã  ajouter

- Interface graphique (`textual`, `gradio`, `streamlit`)
- Historique de conversation sauvegardÃ©
- Analyse directe de code avec coloration syntaxique
